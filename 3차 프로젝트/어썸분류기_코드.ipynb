{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유튜브 랭킹 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# 웹 드라이버 설정 및 페이지 접속\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.set_window_size(1200, 2000)\n",
    "\n",
    "# 빈 리스트 초기화\n",
    "collected_data = []\n",
    "\n",
    "# 페이지 1부터 84까지 순회\n",
    "for page_num in tqdm(range(1, 84), desc=\"Scraping pages\"):\n",
    "    # 페이지 URL 업데이트\n",
    "    page_url = f\"https://youtube-rank.com/board/bbs/board.php?bo_table=youtube&page={page_num}\"\n",
    "    driver.get(page_url)\n",
    "    time.sleep(1)  # 웹 페이지 로드를 위해 대기\n",
    "\n",
    "    # 맨 아래까지 스크롤\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(1)  # 페이지가 완전히 로드될 때까지 기다립니다\n",
    "\n",
    "    # 각 페이지의 제목과 카테고리 추출\n",
    "    for num in range(1, 101):  # 1부터 100까지\n",
    "        try:\n",
    "            titles_xpath = f'//*[@id=\"list-skin\"]/form[1]/table/tbody/tr[{num}]/td[3]/h1/a'\n",
    "            category_xpath = f'//*[@id=\"list-skin\"]/form[1]/table/tbody/tr[{num}]/td[3]/h1/p'\n",
    "\n",
    "            title_element = driver.find_element(By.XPATH, titles_xpath)\n",
    "            category_element = driver.find_element(By.XPATH, category_xpath)\n",
    "\n",
    "            title_text = title_element.text\n",
    "            category_text = category_element.text\n",
    "\n",
    "            # 수집된 데이터 저장\n",
    "            collected_data.append({'title': title_text, 'category': category_text})\n",
    "\n",
    "        except NoSuchElementException:\n",
    "            # 요소가 페이지에 없는 경우\n",
    "            continue\n",
    "\n",
    "# 리스트를 DataFrame으로 변환\n",
    "news_df = pd.DataFrame(collected_data)\n",
    "\n",
    "# 웹 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 결과 출력\n",
    "print(news_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from isodate import parse_duration\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# API 키 리스트\n",
    "api_keys = [\n",
    "    'AIzaSyCVUXop1pTF7x4sP1dIRJk9gVtn5d7MfKQ',\n",
    "    'AIzaSyCwBiqWbx-_0kmU5bi20YgsmijbyhTgi_M',\n",
    "    'AIzaSyDkQaL5xk6FYsTGHv8o5so-Aulmyf5tW3A'\n",
    "]\n",
    "key_index = 0  # 현재 사용 중인 API 키의 인덱스\n",
    "\n",
    "# 현재 사용 중인 API 키 가져오기\n",
    "def get_current_api_key():\n",
    "    return api_keys[key_index]\n",
    "\n",
    "# 다음 API 키로 전환\n",
    "def switch_to_next_api_key():\n",
    "    global key_index\n",
    "    key_index = (key_index + 1) % len(api_keys)\n",
    "\n",
    "# YouTube build 객체 생성\n",
    "youtube = build('youtube', 'v3', developerKey=get_current_api_key())\n",
    "\n",
    "# 'id3.csv'에서 채널명과 카테고리 리스트를 가져오기\n",
    "with open('id3.csv', mode='r', encoding='utf-8-sig') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    channels_info = [(row['title'], row['category']) for row in csv_reader]\n",
    "\n",
    "# CSV 파일 존재 여부 확인 및 헤더 작성\n",
    "csv_file_path = 'youtube_videos.csv'\n",
    "file_exists = os.path.isfile(csv_file_path)\n",
    "\n",
    "# ISO 8601 기간을 초로 변환하는 함수\n",
    "def iso8601_duration_to_seconds(duration):\n",
    "    return int(parse_duration(duration).total_seconds())\n",
    "\n",
    "try:\n",
    "    # 각 채널에 대해 정보 수집\n",
    "    for channel_name, channel_category in channels_info:\n",
    "        while True:\n",
    "            try:\n",
    "                # 채널 검색 및 채널 ID 추출\n",
    "                search_response = youtube.search().list(\n",
    "                    q=channel_name,\n",
    "                    order='relevance',\n",
    "                    part='snippet',\n",
    "                    maxResults=1\n",
    "                ).execute()\n",
    "\n",
    "                channel_id = search_response['items'][0]['id']['channelId'] if search_response['items'] else None\n",
    "\n",
    "                if channel_id:\n",
    "                    # 채널 구독자 수 가져오기\n",
    "                    api_key = get_current_api_key()\n",
    "                    channel_url = f'https://www.googleapis.com/youtube/v3/channels?part=statistics&id={channel_id}&key={api_key}'\n",
    "                    channel_response = requests.get(channel_url)\n",
    "                    channel_data = channel_response.json()\n",
    "                    subscriber_count = channel_data['items'][0]['statistics']['subscriberCount'] if 'items' in channel_data and channel_data['items'] else \"Unknown\"\n",
    "\n",
    "                    # 모든 동영상 정보 가져오기\n",
    "                    page_token = None\n",
    "                    while True:\n",
    "                        search_url = f'https://www.googleapis.com/youtube/v3/search?key={api_key}&channelId={channel_id}&part=snippet,id&order=date&maxResults=50&publishedAfter=2021-01-01T00:00:00Z{f\"&pageToken={page_token}\" if page_token else \"\"}'\n",
    "                        search_response = requests.get(search_url)\n",
    "                        search_data = search_response.json()\n",
    "\n",
    "                        # 동영상 정보 추출 및 저장\n",
    "                        with open(csv_file_path, 'a', newline='', encoding='utf-8') as file:\n",
    "                            writer = csv.writer(file)\n",
    "\n",
    "                            if not file_exists:\n",
    "                                writer.writerow(['Category', 'Video Title', 'View Count', 'Subscriber Count', 'Upload Date', 'Thumbnail URL'])\n",
    "                                file_exists = True\n",
    "\n",
    "                            for video in search_data['items']:\n",
    "                                if video['id']['kind'] == 'youtube#video':\n",
    "                                    video_id = video['id']['videoId']\n",
    "                                    video_title = video['snippet']['title']\n",
    "                                    upload_date = video['snippet']['publishedAt']\n",
    "                                    thumbnail_url = video['snippet']['thumbnails']['high']['url']\n",
    "\n",
    "                                    # 조회수 및 비디오 기간 가져오기\n",
    "                                    video_url = f'https://www.googleapis.com/youtube/v3/videos?part=contentDetails,statistics&id={video_id}&key={api_key}'\n",
    "                                    video_response = requests.get(video_url)\n",
    "                                    video_data = video_response.json()\n",
    "                                    video_duration = video_data['items'][0]['contentDetails']['duration']\n",
    "                                    video_duration_seconds = iso8601_duration_to_seconds(video_duration)\n",
    "                                    view_count = video_data['items'][0]['statistics']['viewCount']\n",
    "\n",
    "                                    # 60초 이상의 동영상만 저장\n",
    "                                    if video_duration_seconds >= 60:\n",
    "                                        writer.writerow([channel_category, video_title, view_count, subscriber_count, upload_date, thumbnail_url])\n",
    "\n",
    "                        page_token = search_data.get('nextPageToken')\n",
    "                        if not page_token:\n",
    "                            break\n",
    "\n",
    "                    print(f\"CSV 파일 {channel_name} 저장 완료\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"Error: Unable to retrieve channel ID for {channel_name}.\")\n",
    "                    break\n",
    "\n",
    "            except KeyError:\n",
    "                print(f\"Channel ID not found for {channel_name}. Skipping to next channel.\")\n",
    "                break\n",
    "\n",
    "            except HttpError as e:\n",
    "                if e.resp.status == 403:\n",
    "                    print(\"API 사용 제한이 감지되었습니다. 다른 API 키로 전환합니다.\")\n",
    "                    switch_to_next_api_key()\n",
    "                    youtube = build('youtube', 'v3', developerKey=get_current_api_key())\n",
    "                else:\n",
    "                    print(f\"YouTube API Error: {e}\")\n",
    "                    break\n",
    "\n",
    "    print(\"Processing completed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 경로\n",
    "csv_file_path = 'file.csv'\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "def download_image(image_url, folder_name, file_id):\n",
    "    # 폴더가 없으면 생성\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "    \n",
    "    # 파일 경로 생성 (폴더명_id.jpg)\n",
    "    file_path = os.path.join(folder_name, f'{folder_name}_{file_id}.jpg')\n",
    "    \n",
    "    # 이미지 다운로드 및 파일에 저장\n",
    "    response = requests.get(image_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    # 카테고리 번호와 클래스 접미사로 폴더명 생성\n",
    "    category_prefix = f'{row[\"Category Number\"]:02d}'\n",
    "    class_suffix = '1'\n",
    "    if row['class_a'] == 1:\n",
    "        class_suffix = '1'\n",
    "    elif row['class_b'] == 1:\n",
    "        class_suffix = '2'\n",
    "    elif row['class_c'] == 1:\n",
    "        class_suffix = '3'\n",
    "    elif row['class_d'] == 1:\n",
    "        class_suffix = '4'\n",
    "    folder_name = f'{category_prefix}{class_suffix}'  # 예: '011'\n",
    "    \n",
    "    # 특정 카테고리 번호 범위에 대해서만 이미지 다운로드\n",
    "    if folder_name.startswith('01'):  # '01'로 시작하는 폴더에 대해서만 작업 실행\n",
    "        # 이미지 URL\n",
    "        image_url = row['Thumbnail URL']\n",
    "        \n",
    "        # 데이터프레임의 고유 식별자(ID)를 파일명에 사용\n",
    "        file_id = row['ID']\n",
    "        \n",
    "        # 이미지 다운로드 함수 호출\n",
    "        download_image(image_url, folder_name, file_id)\n",
    "\n",
    "print(\"이미지 다운로드 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 리사이즈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import splitfolders\n",
    "\n",
    "folder_path = 'img'\n",
    "label_names = os.listdir(folder_path)\n",
    "label_names\n",
    "\n",
    "os.mkdir('resize')\n",
    "for label in label_names:\n",
    "    dir_path = 'resize/'+ label\n",
    "    os.mkdir(dir_path)\n",
    "\n",
    "\n",
    "dataset = {} # 카테고리명 : [이미지1경로, 이미지2경로, ...]\n",
    "\n",
    "for label in os.listdir(folder_path):\n",
    "    sub_path = folder_path+'/'+label+'/'\n",
    "    dataset[label] = []\n",
    "    for filename in os.listdir(sub_path):\n",
    "        dataset[label].append(sub_path+filename)\n",
    "\n",
    "def resize_img(img_path, img_size=224):\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    if(img.shape[1] > img.shape[0]) :\n",
    "        ratio = img_size/img.shape[1]\n",
    "    else :\n",
    "        ratio = img_size/img.shape[0]\n",
    "\n",
    "    img = cv2.resize(img, dsize=(0, 0), fx=ratio, fy=ratio, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    # 그림 주변에 검은색으로 칠하기\n",
    "    w, h = img.shape[1], img.shape[0]\n",
    "\n",
    "    dw = (img_size-w)/2 # img_size와 w의 차이\n",
    "    dh = (img_size-h)/2 # img_size와 h의 차이\n",
    "\n",
    "    M = np.float32([[1,0,dw], [0,1,dh]])  #(2*3 이차원 행렬)\n",
    "    img_re = cv2.warpAffine(img, M, (224, 224)) #이동변환\n",
    "    cv2.imwrite('resize/{0}/{1}'.format(label, img_path.split(\"/\")[-1]) , img_re)\n",
    "\n",
    "for label, img_paths in dataset.items():\n",
    "    for img_path in img_paths:\n",
    "        resize_img(img_path, img_size=224)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 스플릿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio('resize', output='dataset', seed=77, ratio=(0.6, 0.2, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분석 및 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_dataset_files(directory):\n",
    "    category_counts = {}\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if 'train' in root: \n",
    "            for dir in dirs:\n",
    "                path = os.path.join(root, dir)\n",
    "                num_files = len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))])\n",
    "                category_counts[dir] = num_files\n",
    "    return category_counts\n",
    "\n",
    "dataset_directory = 'dataset' \n",
    "counts = count_dataset_files(dataset_directory)\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.core.composition import Compose\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import os\n",
    "\n",
    "# 데이터 증강 파이프라인 정의\n",
    "augmentations = Compose([\n",
    "    A.RandomBrightnessContrast(p=0.5),  # 밝기와 대비를 무작위로 조정\n",
    "    A.Resize(224, 224)  # 224x224로 이미지 크기 조정\n",
    "])\n",
    "\n",
    "def augment_and_save(image_path, save_path, augmentations, num_augmented_images=10):\n",
    "    image = io.imread(image_path)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # 원본 이미지 이름\n",
    "    image_name = os.path.basename(image_path)\n",
    "    \n",
    "    for i in range(num_augmented_images):\n",
    "        # 데이터 증강 적용\n",
    "        augmented = augmentations(image=image)\n",
    "        augmented_image = augmented[\"image\"]\n",
    "        \n",
    "        # 증강된 이미지 저장\n",
    "        augmented_image_path = os.path.join(save_path, f\"aug_{i}_{image_name}\")\n",
    "        io.imsave(augmented_image_path, augmented_image)\n",
    "\n",
    "def augment_images_in_directory(source_dir, save_dir_base, augmentations):\n",
    "    for category in os.listdir(source_dir):\n",
    "        category_dir = os.path.join(source_dir, category)\n",
    "        if os.path.isdir(category_dir):\n",
    "            save_dir = os.path.join(save_dir_base, category)\n",
    "            for image_name in os.listdir(category_dir):\n",
    "                image_path = os.path.join(category_dir, image_name)\n",
    "                augment_and_save(image_path, save_dir, augmentations)\n",
    "\n",
    "# 데이터 증강을 수행할 각 카테고리별 디렉토리의 기본 경로\n",
    "source_dir = 'dataset/train'\n",
    "save_dir_base = 'dataset/augmented'\n",
    "\n",
    "# 모든 카테고리에 대해 데이터 증강 수행\n",
    "augment_images_in_directory(source_dir, save_dir_base, augmentations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 만개로 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "def trim_images_in_folder(folder, max_images=10000):\n",
    "    files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    \n",
    "    if len(files) > max_images:\n",
    "        #파일삭제\n",
    "        num_files_to_remove = len(files) - max_images\n",
    "        files_to_remove = random.sample(files, num_files_to_remove)\n",
    "        \n",
    "        # 선택된 파일 삭제\n",
    "        for file in files_to_remove:\n",
    "            os.remove(file)\n",
    "            print(f\"Removed: {file}\")\n",
    "\n",
    "\n",
    "augmented_folder = 'dataset/augmented'\n",
    "for category_folder in os.listdir(augmented_folder):\n",
    "    folder_path = os.path.join(augmented_folder, category_folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        print(f\"Trimming images in {folder_path}...\")\n",
    "        trim_images_in_folder(folder_path, max_images=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from keras.regularizers import l2  # 여기에 l2를 임포트합니다\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam  # Adam 옵티마이저를 임포트합니다\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 데이터 증강을 위한 설정 (필요에 따라 조정)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "for category in range(1, 17):  # 16개 카테고리\n",
    "    # ResNet50 모델 초기화, Fully Connected Layer 제외\n",
    "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "    # 모델 커스터마이징\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)  \n",
    "    x = Dense(1024, activation='relu', kernel_regularizer=l2(0.001))(x)  \n",
    "    predictions = Dense(4, activation='softmax', kernel_regularizer=l2(0.001))(x)  \n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 조기 종료 콜백 설정\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
    "\n",
    "    # 카테고리별 폴더를 직접 지정\n",
    "    category_folders = [f'{category:02d}{score}' for score in range(1, 5)]\n",
    "\n",
    "    # 훈련 및 검증 데이터 생성기 설정\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/combined',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        classes=category_folders,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        'dataset/val',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=32,\n",
    "        classes=category_folders,\n",
    "        class_mode='categorical')\n",
    "\n",
    "    # 모델 학습\n",
    "    print(f\"Training model for Category {category:02d}\")\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        validation_data=val_generator,\n",
    "        epochs=20,\n",
    "        steps_per_epoch=train_generator.samples // 32,\n",
    "        validation_steps=val_generator.samples // 32,\n",
    "        callbacks=[early_stopping])  \n",
    "\n",
    "    # 학습된 모델 저장\n",
    "    model_save_path = f'saved_models/category_{category:02d}_new_model.h5'\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Trained model for Category {category:02d} saved at {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 'img' 폴더 내의 이미지 파일 이름 가져오기\n",
    "image_folder = 'img'\n",
    "image_files = [f for f in os.listdir(image_folder) if os.path.isfile(os.path.join(image_folder, f))]\n",
    "\n",
    "# 이미지 파일이 있을 경우 첫 번째 이미지 사용\n",
    "if image_files:\n",
    "    image_path = os.path.join(image_folder, image_files[0])  # 첫 번째 이미지의 경로\n",
    "    input_image = load_img(image_path, target_size=(224, 224))  # 모델에 맞는 이미지 크기로 로드\n",
    "    input_image = img_to_array(input_image)  # 이미지를 배열로 변환\n",
    "    input_image = np.expand_dims(input_image, axis=0)  # 모델 예측을 위해 차원 추가\n",
    "    input_image /= 255.0  # 이미지 정규화\n",
    "\n",
    "    # 모든 모델의 예측 확률을 저장할 배열 초기화\n",
    "    all_predictions = np.zeros((16, 4))  # 16개 모델, 각 모델은 4개 클래스에 대한 확률을 예측\n",
    "\n",
    "    # 16개 모델에 대해 반복\n",
    "    for i in range(1, 17):\n",
    "        # 모델 로드\n",
    "        model_path = f'saved_models/category_{i:02d}_new_model.h5'\n",
    "        model = load_model(model_path)\n",
    "\n",
    "        # 이미지에 대한 예측 수행\n",
    "        pred = model.predict(input_image)\n",
    "\n",
    "        # 예측 확률 저장\n",
    "        all_predictions[i-1] = pred\n",
    "\n",
    "        # 모델별 가장 높은 확률을 가진 클래스 출력 \n",
    "        predicted_class = np.argmax(pred) + 1\n",
    "        print(f\"Model {i:02d} predicted class: {predicted_class}\")\n",
    "\n",
    "    # 모든 모델의 예측 확률의 평균 계산\n",
    "    average_predictions = np.mean(all_predictions, axis=0)\n",
    "\n",
    "    # 평균 확률이 가장 높은 클래스 선택\n",
    "    final_prediction = np.argmax(average_predictions) + 1 \n",
    "\n",
    "    if final_prediction == 1:\n",
    "        print(\"썸네일 주목도가 '매우 높음'입니다\")\n",
    "    elif final_prediction == 2:\n",
    "        print(\"썸네일 주목도가 '높음' 입니다\")\n",
    "    elif final_prediction == 3:\n",
    "        print(\"썸네일 주목도가 '보통' 입니다\")\n",
    "    elif final_prediction == 4:\n",
    "        print(\"썸네일 주목도가 '낮음' 입니다\")\n",
    "    else:\n",
    "        print(\"에러\")\n",
    "\n",
    "else:\n",
    "    print(\"Error\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
